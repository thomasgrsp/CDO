{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "src_breast_cancer = 'breast_cancer/wdbc.csv'\n",
    "src_ionosphere = 'ionosphere/ionosphere.csv'\n",
    "\n",
    "bc_data = pd.read_csv(src_breast_cancer, delimiter=',')\n",
    "io_data = pd.read_csv(src_ionosphere, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568,) (568, 30)\n",
      "(350,) (350, 34)\n"
     ]
    }
   ],
   "source": [
    "# Get data as np array and split bc_classes/bc_features\n",
    "bc_classes = bc_data[bc_data.columns[1]].values\n",
    "bc_features = bc_data[bc_data.columns[2:]].values\n",
    "print(bc_classes.shape, bc_features.shape)\n",
    "io_classes = io_data[io_data.columns[-1]].values\n",
    "io_features = io_data[io_data.columns[:-1]].values\n",
    "print(io_classes.shape, io_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of maligne:  211\n",
      "Number of benigne:  0\n",
      "Number of bad:  126\n",
      "Number of good:  0\n"
     ]
    }
   ],
   "source": [
    "# Process bc_features into 0 and 1 class\n",
    "bc_classes[bc_classes == 'M'] = 1\n",
    "bc_classes[bc_classes == 'B'] = -1\n",
    "print('Number of maligne: ', np.count_nonzero(bc_classes == 1))\n",
    "print('Number of benigne: ', np.count_nonzero(bc_classes == 0))\n",
    "bc_classes = bc_classes.astype(np.int8)\n",
    "# Process io_features into 0 and 1 class\n",
    "io_classes[io_classes == 'b'] = 1\n",
    "io_classes[io_classes == 'g'] = -1\n",
    "print('Number of bad: ', np.count_nonzero(io_classes == 1))\n",
    "print('Number of good: ', np.count_nonzero(io_classes == 0))\n",
    "io_classes = io_classes.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-center data\n",
    "bc_features -= np.mean(bc_features, axis=0)\n",
    "io_features -= np.mean(io_features, axis=0)\n",
    "# 1-center std\n",
    "bc_features /= np.std(bc_features, axis=0)\n",
    "io_features = np.divide(io_features, np.std(io_features, axis=0), where=np.std(io_features, axis=0) != 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568,) int8\n",
      "(568, 30)\n",
      "(350,) int8\n",
      "(350, 34)\n"
     ]
    }
   ],
   "source": [
    "print(bc_classes.shape, bc_classes.dtype)\n",
    "print(bc_features.shape)\n",
    "print(io_classes.shape, io_classes.dtype)\n",
    "print(io_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads cross validation framework\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import cross_validation\n",
    "import seaborn as sn\n",
    "bc_xtrain, bc_xtest, bc_ytrain, bc_ytest = train_test_split(bc_features, bc_classes, test_size=.2)\n",
    "io_xtrain, io_xtest, io_ytrain, io_ytest = train_test_split(io_features, io_classes, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def logistic_loss(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    return np.log(1.+np.exp(-label*np.dot(features, x)))+l*np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad(features, label, x, l):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\"\"\"\n",
    "    return (-label*features*np.exp(-label*np.dot(features, x)))/(1.+np.exp(-label*np.dot(features, x)))+2.*l*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    return max(0., 1.-label*np.dot(features, x))+l*np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_grad(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    if 1.-label*np.dot(features, x) < 0.:\n",
    "        return np.zeros(features.shape[0])\n",
    "    else:\n",
    "        return -label*features+2.*l*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_square_loss(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    return max(0., 1.-label*np.dot(features, x))**2+l*np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_square_grad(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    if 1.-label*np.dot(features, x) < 0.:\n",
    "        return np.zeros(features.shape[0])\n",
    "    else:\n",
    "        return -2.*label*features*(1.-label*np.dot(features, x))+2.*l*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def template(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate=1e-3):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = n_epochs // 10\n",
    "    for epoch in range(n_epochs):\n",
    "        ############ A REMPLIR\n",
    "        grad = XXXXX\n",
    "        \n",
    "        x -= learning_rate * grad\n",
    "        \n",
    "        # Compute loss of whole dataset\n",
    "        if epoch % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preds(x, tefeatures):\n",
    "    p = 1. / (1. + np.exp(-np.dot(tefeatures, x)))\n",
    "    p[p < .5] = -1\n",
    "    p[p >= .5] = 1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(real, preds):\n",
    "    return np.sum(real == preds) / float(real.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def ggwp(algo_descent, n_epochs, reg, momentum=None, learning_rate=1e-3):\n",
    "    for loss_type in ((logistic_loss, logistic_grad), (hinge_loss, hinge_grad), (hinge_square_loss, hinge_square_grad)):\n",
    "        for i, dataset in enumerate(((bc_xtrain, bc_ytrain, bc_xtest, bc_ytest), (io_xtrain, io_ytrain, io_xtest, io_ytest))):\n",
    "            loss = loss_type[0]\n",
    "            grad = loss_type[1]\n",
    "            trfeatures, trlabels = dataset[0], dataset[1]\n",
    "            tefeatures, telabels = dataset[2], dataset[3]\n",
    "            x_init = np.zeros(trfeatures[0].shape[0])\n",
    "            print('XXXXXXXXXXXXXXXXXXXXXXX\\nDataset', i, 'Loss type', loss.__name__)\n",
    "            start = time.time()\n",
    "            x = algo_descent(x_init, trfeatures, trlabels, loss, grad, n_epochs, reg, momentum)\n",
    "            print('Time elapsed', time.time() - start)\n",
    "            print('Accuracy', accuracy(telabels, preds(x, tefeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "def batch_gd(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, momentum=None, learning_rate=1e-3):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = 10\n",
    "    for epoch in range(n_epochs):\n",
    "        ############ A REMPLIR\n",
    "        grad = np.mean([gradient_loss_function(f, lab, x, lamb) for f, lab in zip(features, labels)], axis=0)\n",
    "        if momentum:\n",
    "            x = x*momentum - learning_rate*grad\n",
    "        else:\n",
    "            x -= learning_rate * grad\n",
    "        # Compute loss of whole dataset\n",
    "        if epoch+1 % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "def sgd(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, momentum=None,learning_rate=1e-3):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = 10\n",
    "    for epoch in range(n_epochs):\n",
    "        for _ in range(n_samples):\n",
    "            i = np.random.randint(n_samples)\n",
    "            grad = gradient_loss_function(features[i], labels[i], x, lamb)\n",
    "            if momentum:\n",
    "                x = x*momentum - learning_rate*grad\n",
    "            else:\n",
    "                x -= learning_rate * grad\n",
    "        # Compute loss of whole dataset\n",
    "        if epoch+1 % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "def adagrad(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate=1e-3, epsilon=1e-8):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = 101010101010101010\n",
    "    Gt = np.zeros((features[0].shape[0], features[0].shape[0]))\n",
    "    for epoch in range(n_epochs):\n",
    "        for _ in range(n_samples):\n",
    "            i = np.random.randint(n_samples)\n",
    "            grad = gradient_loss_function(features[i], labels[i], x, lamb)\n",
    "            for j, g in enumerate(grad):\n",
    "                Gt[j, j] += g*g\n",
    "            x -= learning_rate / np.sqrt([Gt[v, v] + epsilon for v in range(Gt.shape[0])]) * grad\n",
    "        # Compute loss of whole dataset\n",
    "        if epoch+1 % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type logistic_loss\n",
      "Time elapsed 1.9644689559936523\n",
      "Accuracy 0.938596491228\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type logistic_loss\n",
      "Time elapsed 1.2317419052124023\n",
      "Accuracy 0.771428571429\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_loss\n",
      "Time elapsed 0.8641369342803955\n",
      "Accuracy 0.938596491228\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_loss\n",
      "Time elapsed 0.5703029632568359\n",
      "Accuracy 0.771428571429\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_square_loss\n",
      "Time elapsed 1.5666429996490479\n",
      "Accuracy 0.947368421053\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_square_loss\n",
      "Time elapsed 1.1671841144561768\n",
      "Accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "ggwp(batch_gd, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type logistic_loss\n",
      "Time elapsed 2.859323024749756\n",
      "Accuracy 0.90350877193\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type logistic_loss\n",
      "Time elapsed 1.7488341331481934\n",
      "Accuracy 0.457142857143\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_loss\n",
      "Time elapsed 1.7343401908874512\n",
      "Accuracy 0.929824561404\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_loss\n",
      "Time elapsed 1.0628080368041992\n",
      "Accuracy 0.742857142857\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_square_loss\n",
      "Time elapsed 2.7580130100250244\n",
      "Accuracy 0.921052631579\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_square_loss\n",
      "Time elapsed 1.703728199005127\n",
      "Accuracy 0.528571428571\n"
     ]
    }
   ],
   "source": [
    "ggwp(sgd, 100, 1e-3, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type logistic_loss\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-df75368930f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mggwp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madagrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-757bc54cbf5b>\u001b[0m in \u001b[0;36mggwp\u001b[0;34m(algo_descent, n_epochs, reg, momentum, learning_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XXXXXXXXXXXXXXXXXXXXXXX\\nDataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time elapsed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtefeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-24d048f2980f>\u001b[0m in \u001b[0;36madagrad\u001b[0;34m(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate, epsilon)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mGt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Compute loss of whole dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_print\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "ggwp(adagrad, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
