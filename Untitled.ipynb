{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "src_breast_cancer = 'breast_cancer/wdbc.csv'\n",
    "src_ionosphere = 'ionosphere/ionosphere.csv'\n",
    "\n",
    "bc_data = pd.read_csv(src_breast_cancer, delimiter=',')\n",
    "io_data = pd.read_csv(src_ionosphere, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568,) (568, 30)\n",
      "(350,) (350, 34)\n"
     ]
    }
   ],
   "source": [
    "# Get data as np array and split bc_classes/bc_features\n",
    "bc_classes = bc_data[bc_data.columns[1]].values\n",
    "bc_features = bc_data[bc_data.columns[2:]].values\n",
    "print(bc_classes.shape, bc_features.shape)\n",
    "io_classes = io_data[io_data.columns[-1]].values\n",
    "io_features = io_data[io_data.columns[:-1]].values\n",
    "print(io_classes.shape, io_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of maligne:  211\n",
      "Number of benigne:  0\n",
      "Number of bad:  126\n",
      "Number of good:  0\n"
     ]
    }
   ],
   "source": [
    "# Process bc_features into 0 and 1 class\n",
    "bc_classes[bc_classes == 'M'] = 1\n",
    "bc_classes[bc_classes == 'B'] = -1\n",
    "print('Number of maligne: ', np.count_nonzero(bc_classes == 1))\n",
    "print('Number of benigne: ', np.count_nonzero(bc_classes == 0))\n",
    "bc_classes = bc_classes.astype(np.int8)\n",
    "# Process io_features into 0 and 1 class\n",
    "io_classes[io_classes == 'b'] = 1\n",
    "io_classes[io_classes == 'g'] = -1\n",
    "print('Number of bad: ', np.count_nonzero(io_classes == 1))\n",
    "print('Number of good: ', np.count_nonzero(io_classes == 0))\n",
    "io_classes = io_classes.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-center data\n",
    "bc_features -= np.mean(bc_features, axis=0)\n",
    "io_features -= np.mean(io_features, axis=0)\n",
    "# 1-center std\n",
    "bc_features /= np.std(bc_features, axis=0)\n",
    "io_features = np.divide(io_features, np.std(io_features, axis=0), where=np.std(io_features, axis=0) != 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568,) int8\n",
      "(568, 30)\n",
      "(350,) int8\n",
      "(350, 34)\n"
     ]
    }
   ],
   "source": [
    "print(bc_classes.shape, bc_classes.dtype)\n",
    "print(bc_features.shape)\n",
    "print(io_classes.shape, io_classes.dtype)\n",
    "print(io_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads cross validation framework\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import cross_validation\n",
    "import seaborn as sn\n",
    "bc_xtrain, bc_xtest, bc_ytrain, bc_ytest = train_test_split(bc_features, bc_classes, test_size=.2)\n",
    "io_xtrain, io_xtest, io_ytrain, io_ytest = train_test_split(io_features, io_classes, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def logistic_loss(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    return np.log(1.+np.exp(-label*np.dot(features, x)))+l*np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_grad(features, label, x, l):\n",
    "    \"\"\" Computes the logistic gradient for a Labeled point\"\"\"\n",
    "    return (-label*features*np.exp(-label*np.dot(features, x)))/(1.+np.exp(-label*np.dot(features, x)))+2.*l*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    return max(0., 1.-label*np.dot(features, x))+l*np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_grad(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    if 1.-label*np.dot(features, x) < 0.:\n",
    "        return np.zeros(features.shape[0])\n",
    "    else:\n",
    "        return -label*features+2.*l*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_square_loss(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    return max(0., 1.-label*np.dot(features, x))**2+l*np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_square_grad(features, label, x, l):\n",
    "    \"\"\" Computes the logistic loss for a Labeled point\"\"\"\n",
    "    if 1.-label*np.dot(features, x) < 0.:\n",
    "        return np.zeros(features.shape[0])\n",
    "    else:\n",
    "        return -2.*label*features*(1.-label*np.dot(features, x))+2.*l*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def template(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate=1e-3):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = n_epochs // 10\n",
    "    for epoch in range(n_epochs):\n",
    "        ############ A REMPLIR\n",
    "        grad = XXXXX\n",
    "        \n",
    "        x -= learning_rate * grad\n",
    "        \n",
    "        # Compute loss of whole dataset\n",
    "        if epoch % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preds(x, tefeatures):\n",
    "    p = 1. / (1. + np.exp(-np.dot(tefeatures, x)))\n",
    "    p[p < .5] = -1\n",
    "    p[p >= .5] = 1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(real, preds):\n",
    "    return np.sum(real == preds) / float(real.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def ggwp(algo_descent, n_epochs, reg, learning_rate=1e-3):\n",
    "    for loss_type in ((logistic_loss, logistic_grad), (hinge_loss, hinge_grad), (hinge_square_loss, hinge_square_grad)):\n",
    "        for i, dataset in enumerate(((bc_xtrain, bc_ytrain, bc_xtest, bc_ytest), (io_xtrain, io_ytrain, io_xtest, io_ytest))):\n",
    "            loss = loss_type[0]\n",
    "            grad = loss_type[1]\n",
    "            trfeatures, trlabels = dataset[0], dataset[1]\n",
    "            tefeatures, telabels = dataset[2], dataset[3]\n",
    "            x_init = np.zeros(trfeatures[0].shape[0])\n",
    "            print('XXXXXXXXXXXXXXXXXXXXXXX\\nDataset', i, 'Loss type', loss.__name__)\n",
    "            start = time.time()\n",
    "            x = algo_descent(x_init=x_init,\n",
    "                             features=trfeatures,\n",
    "                             labels=trlabels,\n",
    "                             loss_function=loss, \n",
    "                             gradient_loss_function=grad,\n",
    "                             n_epochs=n_epochs,\n",
    "                             lamb=reg,\n",
    "                             learning_rate=learning_rate)\n",
    "            print('Time elapsed', time.time() - start)\n",
    "            print('Accuracy', accuracy(telabels, preds(x, tefeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "def batch_gd(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate=1e-3, batch_size=None, momentum=None):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = 10\n",
    "    for epoch in range(n_epochs):\n",
    "        ############ A REMPLIR\n",
    "        grad = np.mean([gradient_loss_function(f, lab, x, lamb) for f, lab in zip(features, labels)], axis=0)\n",
    "        if momentum:\n",
    "            x = x*momentum - learning_rate*grad\n",
    "        else:\n",
    "            x -= learning_rate * grad\n",
    "        # Compute loss of whole dataset\n",
    "        if epoch+1 % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type logistic_loss\n",
      "Time elapsed 1.8593499660491943\n",
      "Accuracy 0.938596491228\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type logistic_loss\n",
      "Time elapsed 1.099128007888794\n",
      "Accuracy 0.771428571429\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_loss\n",
      "Time elapsed 0.8316330909729004\n",
      "Accuracy 0.938596491228\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_loss\n",
      "Time elapsed 0.5320870876312256\n",
      "Accuracy 0.771428571429\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_square_loss\n",
      "Time elapsed 1.4813802242279053\n",
      "Accuracy 0.947368421053\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_square_loss\n",
      "Time elapsed 1.0458521842956543\n",
      "Accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "ggwp(batch_gd, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mini-Batch gradient descent\n",
    "import random\n",
    "\n",
    "def minibatch_gd(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate=1e-3, batch_size=5, momentum=None):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = 101010101010101010\n",
    "    for epoch in range(n_epochs):\n",
    "        ############ A REMPLIR\n",
    "        for i in range (0, features.shape[0], batch_size):\n",
    "            ids = np.random.choice(n_samples, batch_size)\n",
    "            features_mini = features[ids]\n",
    "            labels_mini = labels[ids]\n",
    "            grad = np.mean([gradient_loss_function(f, lab, x, lamb) for f, lab in zip(features_mini, labels_mini)], axis=0)\n",
    "            x -= learning_rate * grad\n",
    "        # Compute loss of whole dataset\n",
    "        if epoch+1 % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type logistic_loss\n",
      "Time elapsed 2.7020859718322754\n",
      "Accuracy 0.964912280702\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type logistic_loss\n",
      "Time elapsed 1.7534759044647217\n",
      "Accuracy 0.885714285714\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_loss\n",
      "Time elapsed 1.463737964630127\n",
      "Accuracy 0.973684210526\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_loss\n",
      "Time elapsed 0.9808359146118164\n",
      "Accuracy 0.857142857143\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_square_loss\n",
      "Time elapsed 1.5184569358825684\n",
      "Accuracy 0.947368421053\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_square_loss\n",
      "Time elapsed 1.0904130935668945\n",
      "Accuracy 0.828571428571\n"
     ]
    }
   ],
   "source": [
    "ggwp(minibatch_gd, n_epochs=100, reg=1e-3, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "def sgd(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate=1e-3, momentum=None):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = 10\n",
    "    for epoch in range(n_epochs):\n",
    "        for _ in range(n_samples):\n",
    "            i = np.random.randint(n_samples)\n",
    "            grad = gradient_loss_function(features[i], labels[i], x, lamb)\n",
    "            if momentum:\n",
    "                x = x*momentum - learning_rate*grad\n",
    "            else:\n",
    "                x -= learning_rate * grad\n",
    "        # Compute loss of whole dataset\n",
    "        if epoch+1 % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type logistic_loss\n",
      "Time elapsed 2.5395119190216064\n",
      "Accuracy 0.973684210526\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type logistic_loss\n",
      "Time elapsed 1.7058229446411133\n",
      "Accuracy 0.871428571429\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_loss\n",
      "Time elapsed 1.2503480911254883\n",
      "Accuracy 0.973684210526\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_loss\n",
      "Time elapsed 0.9155521392822266\n",
      "Accuracy 0.9\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type hinge_square_loss\n",
      "Time elapsed 1.4676530361175537\n",
      "Accuracy 0.956140350877\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type hinge_square_loss\n",
      "Time elapsed 1.1749019622802734\n",
      "Accuracy 0.871428571429\n"
     ]
    }
   ],
   "source": [
    "ggwp(sgd, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Adagrad\n",
    "def adagrad(x_init, features, labels, loss_function, gradient_loss_function, n_epochs, lamb, learning_rate=1e-3, momentum=None, epsilon=1e-8):\n",
    "    x = x_init\n",
    "    n_samples = len(features)\n",
    "    n_print = 101010101010101010\n",
    "    Gt = np.zeros((features[0].shape[0], features[0].shape[0]))\n",
    "    for epoch in range(n_epochs):\n",
    "        for _ in range(n_samples):\n",
    "            i = np.random.randint(n_samples)\n",
    "            grad = gradient_loss_function(features[i], labels[i], x, lamb)\n",
    "            for j, g in enumerate(grad):\n",
    "                Gt[j, j] += g*g\n",
    "            x = np.asarray([xi - learning_rate / math.sqrt(Gt[v, v] + epsilon) * g for xi, v, g in zip(x, range(Gt.shape[0]), grad)])\n",
    "        # Compute loss of whole dataset\n",
    "        if epoch+1 % n_print == 0:\n",
    "            loss = np.mean([loss_function(f, l, x, lamb) for f, l in zip(features, labels)])\n",
    "            print('Epoch ', epoch+1, ' Loss: ', loss)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 0 Loss type logistic_loss\n",
      "Time elapsed 5.657454967498779\n",
      "Accuracy 0.938596491228\n",
      "XXXXXXXXXXXXXXXXXXXXXXX\n",
      "Dataset 1 Loss type logistic_loss\n"
     ]
    }
   ],
   "source": [
    "ggwp(adagrad, 100, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
